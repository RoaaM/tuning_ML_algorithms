{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoaaM/tuning_ML_algorithms/blob/main/method_for_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grid Search"
      ],
      "metadata": {
        "id": "ugxDrNiTVQvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKzXwrdhVKoA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
        "              'activation': ['relu', 'logistic'],\n",
        "              'learning_rate_init': [0.001, 0.01, 0.1],\n",
        "              'alpha': [0.0001, 0.001, 0.01]}\n",
        "\n",
        "# Create the MLP classifier\n",
        "mlp = MLPClassifier()\n",
        "\n",
        "# Create the grid search object\n",
        "grid_search = GridSearchCV(mlp, param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "In this example, we define a grid of possible values for the\n",
        "'hidden_layer_sizes', 'activation', 'learning_rate_init', and 'alpha'\n",
        "hyperparameters. Then we create an MLPClassifier object and a GridSearchCV\n",
        "object, passing in the classifier and the parameter grid. The GridSearchCV\n",
        "object is then fit to the training data, and the best parameters and score are\n",
        "printed at the end.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VPgP7osHXzEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Search"
      ],
      "metadata": {
        "id": "UMT4_M0VVUDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Define the parameter grid\n",
        "param_dist = {'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
        "              'activation': ['relu', 'logistic'],\n",
        "              'learning_rate_init': np.logspace(-4, -1, 4),\n",
        "              'alpha': np.logspace(-4, -1, 4)}\n",
        "\n",
        "# Create the MLP classifier\n",
        "mlp = MLPClassifier()\n",
        "\n",
        "# Create the random search object\n",
        "random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=10, cv=5)\n",
        "\n",
        "# Fit the random search to the data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", random_search.best_params_)\n",
        "print(\"Best score: \", random_search.best_score_)\n"
      ],
      "metadata": {
        "id": "jd2ZPubnVW17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In this example, we define a distribution of possible values for the\n",
        "'hidden_layer_sizes', 'activation', 'learning_rate_init', and 'alpha' hyperparameters, then we use RandomizedSearchCV to randomly sample from these\n",
        "distributions. The 'n_iter' parameter determines how many random combinations\n",
        "of parameters are tried. The rest of the process is similar to the GridSearchCV example above.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YDvNWmaSYHod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3- BayesSearchCV"
      ],
      "metadata": {
        "id": "E5vG7SizV599"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import BayesSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Define the parameter space\n",
        "param_space = {'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
        "              'activation': ['relu', 'logistic'],\n",
        "              'learning_rate_init': Real(0.0001, 0.1, 'log-uniform'),\n",
        "              'alpha': Real(0.0001, 0.1, 'log-uniform')}\n",
        "\n",
        "# Create the MLP classifier\n",
        "mlp = MLPClassifier()\n",
        "\n",
        "# Create the Bayesian search object\n",
        "opt = BayesSearchCV(mlp, param_space, n_iter=10, cv=5)\n",
        "\n",
        "# Fit the Bayesian search to the data\n",
        "opt.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", opt.best_params_)\n",
        "print(\"Best score: \", opt.best_score_)\n"
      ],
      "metadata": {
        "id": "ib5Tv5j6V7bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In this example, we define the parameter space using the same format as in the previous examples, but using Real class to specify the range of the variables and the sampling method. We use the BayesSearchCV class to perform Bayesian optimization, passing in the MLP classifier, the parameter space, the number of iterations and the cross-validation fold. The rest of the process is similar to the other examples above.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VWt0d1kOYZ4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4- genetic algorithm"
      ],
      "metadata": {
        "id": "F-atRIrtWuau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deap import base, creator, tools, algorithms\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Define the fitness function\n",
        "def fitness_function(individual):\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=individual[:3], activation=individual[3],\n",
        "                        learning_rate_init=individual[4], alpha=individual[5])\n",
        "    mlp.fit(X_train, y_train)\n",
        "    return mlp.score(X_test, y_test),\n",
        "\n",
        "# Define the parameter bounds\n",
        "param_bounds = [((10, 50, 100),), ('relu', 'logistic'), (0.0001, 0.1), (0.0001, 0.1)]\n",
        "\n",
        "# Create the genetic algorithm objects\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_bool\", np.random.choice, param_bounds)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=6)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"evaluate\", fitness_function)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "# Run the genetic algorithm\n",
        "pop = toolbox.population(n=50)\n",
        "hof = tools.HallOfFame(1)\n",
        "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats.register(\"avg\", np.mean)\n",
        "stats.register(\"std\", np.std)\n",
        "stats.register(\"min\", np.min)\n",
        "stats.register(\"max\", np.max)\n",
        "\n",
        "pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=50, stats=stats, halloffame=hof)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", hof[0])\n",
        "print(\"Best score: \", fitness_function(hof[0])[0])\n"
      ],
      "metadata": {
        "id": "zDbdZ-MnWvst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "This is a basic example of how a genetic algorithm can be used to optimize the hyperparameters of a neural network.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "iyiKww4kYxQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5-Particle Swarm Optimization (PSO)"
      ],
      "metadata": {
        "id": "KBUpHq4GWyTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pyswarms as ps\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Define the fitness function\n",
        "def fitness_function(x):\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=x[:, :3], activation=x[:, 3],\n",
        "                        learning_rate_init=x[:, 4], alpha=x[:, 5])\n",
        "    mlp.fit(X_train, y_train)\n",
        "    return -mlp\n"
      ],
      "metadata": {
        "id": "zje-ehG3W4rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "this is an example of how you might use Particle Swarm Optimization (PSO) to tune the hyperparameters of a neural network using the PySwarms library\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yT6sidBdY7Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6- CMA-ES (Covariance Matrix Adaptation Evolution Strategy)"
      ],
      "metadata": {
        "id": "XlhuFU9JXSxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cma\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Define the fitness function\n",
        "def fitness_function(x):\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=x[:3], activation=x[3],\n",
        "                        learning_rate_init=x[4], alpha=x[5])\n",
        "    mlp.fit(X_train, y_train)\n",
        "    return -mlp.score(X_test, y_test)\n",
        "\n",
        "# Define the initial search point and search space bounds\n",
        "x0 = np.array([50, 50, 50, 'relu', 0.01, 0.01])\n",
        "bounds = [(10, 100), (10, 100), (10, 100), ('relu', 'logistic'), (0.0001, 0.1), (0.0001, 0.1)]\n",
        "\n",
        "# Run the CMA-ES optimization\n",
        "res = cma.fmin(fitness_function, x0, 0.2, options={'bounds': bounds, 'verbose': -9})\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", res[0])\n",
        "print(\"Best score: \", -res[1])\n"
      ],
      "metadata": {
        "id": "uCwtEg1bXVGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In this example, we use the cma.fmin() function to perform CMA-ES optimization, passing in the fitness function, the initial search point and the search space bounds. The verbose option is set to -9 to reduce the amount of output generated by the algorithm.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "b06qtBdsZHGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7- grid search with early stopping method"
      ],
      "metadata": {
        "id": "bZjEkYVWZ7Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
        "              'activation': ['relu', 'logistic'],\n",
        "              'learning_rate_init': [0.001, 0.01, 0.1],\n",
        "              'alpha': [0.0001, 0.001, 0.01]}\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
        "\n",
        "# Define the model\n",
        "def create_model(hidden_layer_sizes, activation, learning_rate_init, alpha):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden_layer_sizes, input_dim=X_train.shape[1], activation=activation))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create the grid search object\n",
        "grid_search = GridSearchCV(KerasClassifier(create_model, epochs=100, batch_size=32,\n",
        "                                           verbose=0, callbacks=[early_stopping]),\n",
        "                           param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "sArNdSYJZI7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8- random search with early stopping method"
      ],
      "metadata": {
        "id": "4-xeHPevaCDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Define the parameter grid\n",
        "param_dist = {'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
        "              'activation': ['relu', 'logistic'],\n",
        "              'learning_rate_init': np.logspace(-4, -1, 4),\n",
        "              'alpha': np.logspace(-4, -1, 4)}\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
        "\n",
        "# Define the model\n",
        "def create_model(hidden_layer_sizes, activation, learning_rate_init, alpha):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden_layer_sizes, input_dim=X_train.shape[1], activation=activation))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create the random search object\n",
        "random_search = RandomizedSearchCV\n"
      ],
      "metadata": {
        "id": "YZklWuJwaClt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9- Optuna library to perform hyperparameter optimization of a neural network using the Keras library"
      ],
      "metadata": {
        "id": "XdCC_libaugW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "\n",
        "def create_model(trial):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(trial.suggest_int(\"units_1\", 32, 256), activation=trial.suggest_categorical(\"activation_1\", [\"relu\", \"tanh\"]), input_shape=(784,)))\n",
        "    model.add(Dropout(trial.suggest_uniform(\"dropout_1\", 0.0, 0.5)))\n",
        "    model.add(Dense(trial.suggest_int(\"units_2\", 32, 256), activation=trial.suggest_categorical(\"activation_2\", [\"relu\", \"tanh\"])))\n",
        "    model.add(Dropout(trial.suggest_uniform(\"dropout_2\", 0.0, 0.5)))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=trial.suggest_categorical(\"optimizer\", [\"adam\", \"sgd\"]), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def objective(trial):\n",
        "    model = create_model(trial)\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val, y_val))\n",
        "    val_loss, val_acc = model.evaluate(X_val, y_val)\n",
        "    return val_loss\n",
        "\n",
        "study = optuna.create_study()\n",
        "study.optimize(objective, n_trials=100)\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n"
      ],
      "metadata": {
        "id": "GlXISh8ZavqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "This is an example of how you might use Optuna to perform hyperparameter optimization of a neural network using the Keras library. In this example, we're using a sequential model with two layers. The number of units, the dropout rate and the activation functions are all set as suggested by Optuna, which uses a Tree-structured Parzen Estimator (TPE) algorithm to optimize the hyperparameters.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2Q1wkmcva_MC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10- Keras Tuner to perform hyperparameter tuning of a neural network using the Keras library"
      ],
      "metadata": {
        "id": "pZJpnRYwbOn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kerastuner as kt\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "x_val = x_val.reshape(10000, 784).astype('float32') / 255\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n",
        "\n",
        "def model_builder(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hp.Int('units_1', min_value=32, max_value=256, step=32), input_shape=(784,), activation='relu'))\n",
        "    model.add(Dense(hp.Int('units_2', min_value=32, max_value=256, step=32), activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=hp.Choice('optimizer', ['adam', 'sgd']), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tuner = kt.Hyperband(model_builder, objective='val_acc', max_epochs=10, factor=3, directory='my_dir', project_name='intro_to_kt')\n",
        "\n",
        "tuner.search(x_train, y_train, validation_data=(x_val, y_val))\n",
        "\n",
        "best_model = tuner.get_best_models(1)[0]\n",
        "best_model.summary()\n"
      ],
      "metadata": {
        "id": "JXWJH-4zbBFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In this example, we're using a Sequential model with two layers. The number of units and the optimizer are defined as hyperparameters that are tuned by the Keras Tuner. The Hyperband algorithm is used to perform the tuning.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "v4AWJMaacryX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11- Hyperopt"
      ],
      "metadata": {
        "id": "VbX8TdS8bpYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import fmin, tpe, hp\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "x_val = x_val.reshape(10000, 784).astype('float32') / 255\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n",
        "\n",
        "space = {\n",
        "    'units_1': hp.quniform('units_1', 32, 256, 32),\n",
        "    'units_2': hp.quniform('units_2', 32, 256, 32),\n",
        "    'dropout_1': hp.uniform('dropout_1', 0, 0.5),\n",
        "    'dropout_2': hp.uniform('dropout_2', 0, 0.5),\n",
        "    'optimizer': hp.choice('optimizer', ['adam', 'sgd']),\n",
        "}\n",
        "\n",
        "def train_and_evaluate_model(params):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(params['units_1'], input_shape=(784,), activation='relu'))\n",
        "    model.add(Dropout(params['dropout_1']))\n",
        "    model.add(Dense(params['units_2'], activation='relu'))\n",
        "    model.add(Dropout(params['dropout_2']))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=params['optimizer'], loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_val, y_val))\n",
        "    val_loss, val_acc = model.evaluate(x_val, y_val, verbose=0)\n",
        "    return {'loss': -val_acc, 'status': 'ok'}\n",
        "\n",
        "best = fmin(train_and_evaluate_model, space, algo=tpe.suggest, max_evals=50)\n",
        "print(best)\n"
      ],
      "metadata": {
        "id": "iFdI00_acQzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In this example, we're using a Sequential model with two layers. The number of units, the dropout rate and the optimizer are defined as hyperparameters that are optimized by Hyperopt using the Tree-structured Parzen Estimator (TPE) algorithm.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "oVcfcVJGcmFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12- autokeras\n"
      ],
      "metadata": {
        "id": "5x55pg2_eLSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autokeras"
      ],
      "metadata": {
        "id": "R8Ow8aH2cn5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import autokeras as ak\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "x_val = x_val.reshape(10000, 784).astype('float32') / 255\n",
        "\n",
        "clf = ak.ImageClassifier(verbose=True)\n",
        "clf.fit(x_train, y_train, time_limit=12*60*60)\n",
        "print(clf.evaluate(x_val, y_val))\n"
      ],
      "metadata": {
        "id": "z_KIZ0FVd_sz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}